#!/usr/bin/env python3
# attempt to infer a table DDL from a tsv file

# perl -p0e 's/&gt;/>/g; s/\t-\t/\t\t/g; s/&#39;/\x27/g' variant_results.tsv >|variant_results2.tsv
# tsv-to-sql -l10000000 -N - variant_results2.tsv 
# psql <variant_results2.sql
# psql -c "\\copy variant_results2 from 'variant_results2.tsv' csv delimiter '$t' null '-' header;" 


import argparse
import collections
import csv
import logging
import os
import re
import sys

from namedlist import namedlist


class TypeSniffer(object):
    date_re = re.compile('^(?:\d{4}-\d{2}-\d{2}|\d{1,2}/\d{1,2}/\d{4})$')
    type_tests = collections.OrderedDict([
        ('int',   lambda v: int(v)),
        ('float', lambda v: float(v)),
        ('date',  lambda v: TypeSniffer.date_re.match(v)),
        ('text',  lambda v: True),
        ])
    types = type_tests.keys()

    @classmethod
    def atypify(cls,v,to_try=types):
        "return list of types in to_try that v is not (i.e., excludes possible types)"
        assert isinstance(v,str), "string expected"
        def _could_be_a(t,v):
            try:
                if cls.type_tests[t](v) is None:
                    return False
            except ValueError:
                return False
            return True
        return [ t for t in to_try if not _could_be_a(t,v) ]


class ColDef(namedlist('ColDef',['name','possible_types','has_nulls','sample_values'])):
    def __init__(self,
                 name=None,
                 possible_types=None,
                 has_nulls=False,
                 sample_values=None):
        if possible_types is None:
            possible_types = set(TypeSniffer.types)
        if sample_values is None:
            sample_values = set()
        super(ColDef,self).__init__(name,possible_types,has_nulls,sample_values)

    def update(self,v):
        if not v:
            self.has_nulls = True
            return
        assert isinstance(v,str), "string expected"
        for t in TypeSniffer.atypify(v,to_try=self.possible_types):
            self.possible_types.remove(t)
        self.sample_values.add(v)

    @property
    def sql_column_name(self):
        return sql_tokenify(self.name.strip())

    def as_column_ddl(self):
        return '{col:15s} {type:15s} {opts}'.format(
            col='"'+self.sql_column_name+'"',
            type=self.inferred_type,
            opts=' '.join(self.opts))

    @property
    def inferred_type(self):
        assert len(self.possible_types)>0, "all possible column types eliminated?!"
        if len(self.possible_types - set(['text'])) > 1:
            logging.warn("column '{self.name}' has {np} possible types ({self.possible_types}); returning first".format(
                self=self,np=len(self.possible_types)))
        return sorted(list(self.possible_types))[0]

    @property
    def opts(self):
        o = []
        if not self.has_nulls:
            o.append('NOT NULL')
        return o



def sql_tokenify(s):
    return re.sub('\W','_',s.lower())

def parse_args(argv):
    ap = argparse.ArgumentParser(
        description = __doc__
        )
    ap.add_argument('FILE')
    ap.add_argument('--max-lines', '-l',
                    help='number of lines to sniff',
                    type=int,
                    default=None,
                    )
    ap.add_argument('--null-string','-N')
    ap.add_argument('--table-name','-t')
    opts = ap.parse_args(argv)
    return opts

def infer_column_definitions(fn,opts):
    "returns a list of SQL column definitions based on file header and reading rows"

    fd = sys.stdin if fn == '-' else open(fn,'r')
    tsv_in = csv.DictReader(fd, delimiter='\t')

    coldefs = collections.OrderedDict( (fld,ColDef(name=fld))
                                       for fld in tsv_in.fieldnames )

    for i,rec in enumerate(tsv_in):
        if opts.max_lines and i > opts.max_lines:
            break
        for fld,val in rec.items():
            coldefs[fld].update(None if val == opts.null_string else val)

    return coldefs


if __name__ == '__main__':
    logging.basicConfig(level=logging.WARN)

    opts = parse_args(sys.argv[1:])
    coldefs = infer_column_definitions(opts.FILE,opts)
    
    table_name = opts.table_name or sql_tokenify(os.path.basename(opts.FILE).rsplit('.',1)[0])
    print( "CREATE TABLE " + table_name + " (\n")
    print( "  " + ",\n  ".join([cd.as_column_ddl() for cd in coldefs.values()]) + "\n" )
    print( ");\n" )
