#!/usr/bin/env python3
"""summarize activity in named repos

pip install coloredlogs maya git-python ipython

"""

import argparse
import csv
import datetime
import itertools
import logging
import math
import os
import pprint
import re
import subprocess
import sys

import coloredlogs
import git
import maya


_logger = logging.getLogger(__name__)


def get_options(argv):
    ap = argparse.ArgumentParser(
        description=__doc__)

    ap.add_argument("PATHS",
                    nargs="+")
    ap.add_argument(
        "--from-timestamp", "-F",
        required=False,
        default=None)
    ap.add_argument(
        "--to-timestamp", "-T",
        required=False,
        default=None)

    opts = ap.parse_args(argv)

    if opts.from_timestamp:
        m = re.match(r"(?P<n>\d+)(?P<unit>[dhmwy])", opts.from_timestamp)
        if m:
            unit_map = {'d': 'days', 'h': 'hours', 'm': 'minutes', 'w': 'weeks', 'y': 'years'}
            n, unit = m.groups()
            delta = {unit_map[unit]: -abs(int(n))}
            opts.from_timestamp = maya.now().add(**delta)
            _logger.info(f"{m.string} => {opts.from_timestamp}")
        else:
            opts.from_timestamp = maya.when(opts.from_timestamp).date
            
    if opts.to_timestamp:
        opts.to_timestamp = maya.when(opts.to_timestamp).date

    return opts


def estimated_time(cmt):
    """estimate time taken for commit"""
    i = cmt.stats.total["insertions"]
    d = cmt.stats.total["deletions"]
    f = cmt.stats.total["files"]
    deltalines = abs(i - d)
    if deltalines == 0:
        time = cmt.stats.total["insertions"] / 50
    else:
        time = deltalines / 200 + (f-1) ** 0.2
        if len(cmt.parents) > 1:              # merge
            time = math.log(time)
        time = max(0.5, time) 
    return time


def _build_commit_branch_map(repo, branch_names=None):
    """return map of commit hash: [repo names]"""
    branches = repo.branches
    if branch_names:
        branches = [b for b in branches if b.name in branch_names]
    cb_pairs = sorted(list((c.hexsha,b.name) for b in branches for c in
                            repo.iter_commits(b)))
    cb_map = {c: [cb[1] for cb in cbi]
              for c, cbi in itertools.groupby(cb_pairs, lambda cb: cb[0])}
    return cb_map


def commit_summaries(path, opts):
    """yields dictionaries of commit information for given repo path

    """
    repo = git.Repo(path)
    basename = os.path.basename(os.path.abspath(path))
    cb_map = _build_commit_branch_map(repo)

    for cmt in repo.iter_commits(rev=repo.branches):
        cmt_ts = maya.MayaDT(cmt.authored_date)
        if opts.from_timestamp and cmt_ts < opts.from_timestamp:
            continue
        if opts.to_timestamp and date >= opts.to_timestamp:
            continue
        if "reece" not in str(cmt.author.author()).lower():
            _logger.warn(f"Skipping commit {cmt.hexsha} by {cmt.author}")
            continue

        # branches related to this commit, master sorted last
        branches = cb_map[cmt.hexsha]
        branches.sort(key=lambda e: e == "master")

        d = {
            "date": cmt_ts.local_datetime().date(),
            "repo": basename,
            "commit": f"{cmt.hexsha[:8]}",
            "nfiles": cmt.stats.total["files"],
            "del": cmt.stats.total["deletions"],
            "ins": cmt.stats.total["insertions"],
            "time": estimated_time(cmt),
            "message": cmt.summary,
            "branches": branches,
            }
        yield d


def aggregate_commits_daily(commits):
    commits = sorted(commits, key=lambda e: (e["date"], e["repo"]))
    for date, dcommits in itertools.groupby(commits, key=lambda e: e["date"]):
        dcommits = [dc for dc in dcommits if dc["time"] > 0]
        #if (dc["nfiles"] > 1
        #        and (dc["ins"] >= 4 or dc["del"] >= 4)
        #        and not any(w in dc["message"].lower() for w in ("minor",)))
        commit_messages = [f"â€¢ {dc['repo']}/{dc['branches'][0]}: {dc['message']} ({dc['commit']})" for dc in dcommits]
        messages = f"{len(dcommits)} commits:\n" + "\n".join(commit_messages)
        yield {
            "date": date,
            "tfiles": sum(dc["nfiles"] for dc in dcommits),
            "tdel": sum(dc["del"] for dc in dcommits),
            "tins": sum(dc["ins"] for dc in dcommits),
            "time": round(sum(dc["time"] for dc in dcommits) * 2) / 2.0,  # nearest 1/2 hour
            "messages": messages
            }
            


if __name__ == "__main__":
    coloredlogs.install(level="INFO", logger=_logger)

    opts = get_options(sys.argv[1:])

    # commits = itertools.chain.from_iterable(commit_summaries(path, opts) for path in opts.PATHS)
    commits = []
    for path in opts.PATHS:
        try:
            commits += list(commit_summaries(path, opts))
        except Exception as e:
            _logger.critical(f"Failed to fetch commits for {path}")
            raise e
    daily_commits = aggregate_commits_daily(commits)

    #fieldnames = "week date repo commit nfiles del ins message".split()
    fieldnames = "date tfiles tdel tins time messages".split()
    ofh = csv.DictWriter(sys.stdout, fieldnames=fieldnames, delimiter="\t")
    ofh.writeheader()
    ofh.writerows(daily_commits)
