#!/usr/bin/env python3
"""export mendeley docs in folder (-f) to folders in the filer path (-d)"""

from __future__ import print_function

import argparse
import os
import logging
import shutil
import sqlite3
import sys
import urllib.parse


select_documents_sql = """
select distinct D.id,pmid,title,FI.localUrl,FI.hash
from Folders FO
join DocumentFolders DFO on FO.id=DFO.folderId
join Documents D on DFO.documentId=D.id
join DocumentFiles DFI on D.id=DFI.documentId
join Files FI on DFI.hash=FI.hash
"""

preds = ["D.deduplicated = 'false'"]

select_documents_by_folder_sql = select_documents_sql + "where " + " AND ".join(preds + ["FO.name=:folder"])
select_documents_by_bn_sql     = select_documents_sql + "where " + " AND ".join(preds + ["localUrl like :like_bn"])


def parse_args(argv):
    parser = argparse.ArgumentParser(
        description='copy files in Mendeley into folders')

    parser.add_argument('--database-path', '--db',
                        default = os.path.expanduser(
                            '~/.local/share/data/Mendeley Ltd./Mendeley Desktop/{opts.username}@www.mendeley.com.sqlite'),
                            help='Mendeley database file')
    parser.add_argument('--folder', '-F',
                        help = 'source folder')
    parser.add_argument('--tag', '-T',
                        default = '',
                        help='source tag')
    parser.add_argument('--username', '-U',
                        default = 'reece@harts.net')
    parser.add_argument('--destination-directory', '-d',
                        default = os.path.expanduser('~/Documents/papers/'),
                        help = 'destination directory')
    opts = parser.parse_args()

    return opts


def fetch_documents(cur,opts):
    cur.execute("select * from Folders FO where FO.name=?",(opts.folder,))
    if cur.fetchone is None:
        raise RuntimeError("Folder '{folder}' doesn't exist".format(folder=opts.folder))

    cur.execute(select_documents_by_folder_sql, {'folder': opts.folder})
    rows = cur.fetchall()
    n_sel = len(rows)
    rows = list( filter( lambda row: row['localUrl'].endswith('.pdf'), rows ) )
    n_pdf = len(rows)
    rows = list( filter( lambda row: row['localUrl'].startswith('file://'), rows ) )
    n_file = len(rows)

    logging.info('selected {n_docs} from folder {opts.folder} (sel: {n_sel}, pdf: {n_pdf}, file: {n_file})'.format(
        n_docs = len(rows), opts = opts, n_sel = n_sel, n_pdf = n_pdf, n_file = n_file))

    return rows


if __name__ == '__main__':
    logging.basicConfig(level=logging.INFO,
                        format='%(asctime)s %(levelname)s %(name)s %(message)s')

    opts = parse_args(sys.argv[1:])
    assert any([opts.folder, opts.tag]), "must specify folder or tag"
    opts.database_path = opts.database_path.format(opts=opts)

    dst_dir = os.path.join(opts.destination_directory, opts.folder)
    if not os.path.exists(dst_dir):
        os.makedirs(dst_dir)

    existing_files = set(os.listdir(dst_dir))
    logging.info( '%d pre-existing docs in %s' % (
        len(existing_files), dst_dir) )

    #def _row_dict_factory(cur, row):
    #    return dict( (d[0],row[i]) for i,d in enumerate(cur.description) )
    conn = sqlite3.connect(opts.database_path)
    conn.row_factory = sqlite3.Row
    #conn.row_factory = _row_dict_factory
    cur = conn.cursor()
    logging.info('Opened and created cursor for {opts.database_path}'.format(opts=opts))

    docs = fetch_documents(cur,opts)
    for doc in docs:
        src = urllib.parse.unquote(doc['localUrl'][len('file://'):])
        src_bn = os.path.basename(src)
        dst = os.path.join(dst_dir,src_bn)

        if not os.path.exists(dst):
            try:
                shutil.copy(src,dst)
                logging.info('%s -> %s' % (src,dst))
            except IOError as ex:
                logging.error(ex)
                continue

        try:
            existing_files.remove(src_bn)
        except KeyError:
            pass


    logging.info( '%d pre-existing docs in %s are not in folder %s' % (
        len(existing_files), dst_dir, opts.folder) )

    for bn in existing_files:
        cur.execute(select_documents_by_bn_sql, {
            'like_bn': '%/' + urllib.parse.quote(bn)
            })
        if cur.rowcount == -1:
            logging.warn("%s not found in database; skipping" % bn)
            continue
        if cur.rowcount > 1:
            logging.warn("%: %d occurrences in database; like, weird" % (bn, cur.rowcount))
            continue
        rec = cur.fetchone()
        #os.remove(bn)
        logging.info('deleted {fn}'.format(fn = bn))
